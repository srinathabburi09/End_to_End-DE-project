# End_to_End-DE-project
End_to_End Data Engineering Project


ğŸ“‚ Project Overview
This project implements an end-to-end streaming ETL pipeline for processing Netflix titles data, leveraging Azure Data Factory (ADF) for orchestration and Databricks for transformation and enrichment. The pipeline follows a medallion architecture: Raw â†’ Bronze â†’ Silver â†’ Gold.



ğŸ“ Directory Structure
.
â”œâ”€â”€ pipeline1_support_live/           # ADF pipeline support files
â”‚   â”œâ”€â”€ pipeline/pipeline1.json       # ADF pipeline definition
â”‚   â”œâ”€â”€ dataset/                      # Datasets used in ADF
â”‚   â”œâ”€â”€ linkedService/               # Data lake & GitHub connections
â”‚
â”œâ”€â”€ netflix_project/                 # Databricks notebooks
â”‚   â”œâ”€â”€ 1.AutoLoader.py              # Raw â†’ Bronze (streaming ingest)
â”‚   â”œâ”€â”€ 2_silver.py                  # Bronze â†’ Silver (cleansing)
â”‚   â”œâ”€â”€ 3.Lookup notebook.py         # Reference data mapping
â”‚   â”œâ”€â”€ 4_silver.py                  # Additional Silver transformations
â”‚   â”œâ”€â”€ 5_lookupNotebook.py          # Dimension lookups
â”‚   â”œâ”€â”€ 6.weekday.py                 # Adds weekday column
â”‚   â”œâ”€â”€ DLT notebook - Gold Layer.py # Silver â†’ Gold layer aggregation


âš™ï¸ Technologies Used
	â€¢	Azure Data Lake Storage (Gen2) â€“ for storing raw/bronze/silver/gold data
	â€¢	Azure Data Factory â€“ for triggering pipelines & activities
	â€¢	Databricks (PySpark) â€“ for notebook-based transformations
	â€¢	Structured Streaming â€“ for ingesting new CSVs via AutoLoader
	â€¢	Delta Lake â€“ for reliable, ACID-compliant table storage
	â€¢	GitHub Integration â€“ for version control of linked services and datasets

â¸»

ğŸ”„ Pipeline Flow

1. Raw â†’ Bronze
	â€¢	Handled via 1.AutoLoader.py
	â€¢	Uses spark.readStream with .option("cloudFiles.format", "csv")
	â€¢	Writes output to bronze layer in Delta format

2. Bronze â†’ Silver
	â€¢	Files: 2_silver.py, 4_silver.py
	â€¢	Cleans nulls, splits title, casts columns
	â€¢	Enriches data with additional fields like Shorttitle, duration_minutes, etc.

3. Lookups
	â€¢	Files: 3.Lookup notebook.py, 5_lookupNotebook.py
	â€¢	Adds additional metadata using reference data
	â€¢	E.g., country mappings or rating categories

4. Silver â†’ Gold
	â€¢	File: DLT notebook - Gold Layer.py
	â€¢	Aggregates by type, computes counts, and prepares final analytical output

5. Extras
	â€¢	6.weekday.py: Adds weekday column to analyze release patterns

ğŸ“‚ Azure Data Factory Integration

The folder pipeline1_support_live/ contains:
	â€¢	pipeline1.json â€“ full ADF pipeline (ForEach, Web Activity, etc.)
	â€¢	Linked services for:
	â€¢	Azure Data Lake (DataLake_connection.json)
	â€¢	GitHub (github_connection.json)
	â€¢	Datasets including:
	â€¢	Validation
	â€¢	GitHub-hosted resources
	â€¢	Sink dataset definitions

â¸»

ğŸš€ How to Run
	1.	Deploy ADF pipeline using the exported JSON.
	2.	Upload raw Netflix CSVs to your Data Lake (raw container).
	3.	Execute Databricks notebooks in order:
	â€¢	1.AutoLoader.py
	â€¢	2_silver.py and 4_silver.py
	â€¢	3.Lookup notebook.py, 5_lookupNotebook.py
	â€¢	DLT notebook - Gold Layer.py
	4.	Visualize gold layer in Power BI or Databricks SQL.

â¸»

ğŸ“Œ Best Practices Followed
	â€¢	Medallion architecture with modular notebooks
	â€¢	Autoloader for real-time streaming ingestion
	â€¢	Delta format for reliability and performance
	â€¢	Parameterized lookups and joins for maintainability

â¸»

ğŸ“ Optional Files
	â€¢	manifest.mf: Manifest file (can be ignored for now)

â¸»

ğŸ™Œ Author

Srinath Abburi
Passionate about cloud data engineering, streaming pipelines, and real-time analytics.
ğŸ“§ Feel free to connect on gmail: abburisrinath09@gmail.com or LinkedIn : https://www.linkedin.com/in/abburi-srinath-59a505281/
