# ðŸ“¦ End_to_End-DE-project  
**End-to-End Data Engineering Project using Azure & Databricks**

---

## ðŸ“‚ Project Overview

This project implements an **end-to-end streaming ETL pipeline** for processing Netflix titles data, leveraging **Azure Data Factory (ADF)** for orchestration and **Databricks** for transformation and enrichment. The pipeline follows a **medallion architecture**:  
**Raw â†’ Bronze â†’ Silver â†’ Gold**

---

## ðŸ“ Directory Structure

```
.
â”œâ”€â”€ pipeline1_support_live/           # ADF pipeline support files
â”‚   â”œâ”€â”€ pipeline/pipeline1.json       # ADF pipeline definition
â”‚   â”œâ”€â”€ dataset/                      # Datasets used in ADF
â”‚   â”œâ”€â”€ linkedService/                # Data lake & GitHub connections
â”‚
â”œâ”€â”€ netflix_project/                  # Databricks notebooks
â”‚   â”œâ”€â”€ 1.AutoLoader.py               # Raw â†’ Bronze (streaming ingest)
â”‚   â”œâ”€â”€ 2_silver.py                   # Bronze â†’ Silver (cleansing)
â”‚   â”œâ”€â”€ 3.Lookup notebook.py          # Reference data mapping
â”‚   â”œâ”€â”€ 4_silver.py                   # Additional Silver transformations
â”‚   â”œâ”€â”€ 5_lookupNotebook.py           # Dimension lookups
â”‚   â”œâ”€â”€ 6.weekday.py                  # Adds weekday column
â”‚   â”œâ”€â”€ DLT notebook - Gold Layer.py  # Silver â†’ Gold layer aggregation
```


---

## âš™ï¸ Technologies Used

- **Azure Data Lake Storage Gen2** â€“ Storing raw, bronze, silver, and gold data  
- **Azure Data Factory (ADF)** â€“ Orchestration using pipelines and activities  
- **Azure Databricks (PySpark)** â€“ Transformations and streaming ingestion  
- **Structured Streaming + AutoLoader** â€“ Real-time ingestion of new CSVs  
- **Delta Lake** â€“ ACID-compliant data storage format  
- **GitHub Integration** â€“ Version control for pipelines and notebooks  

---

## ðŸ”„ Pipeline Flow

### 1. ðŸ”¹ Raw â†’ Bronze
- Handled via `1.AutoLoader.py`  
- Uses `spark.readStream` with `cloudFiles.format = "csv"`  
- Writes ingested data in Delta format to Bronze layer

### 2. ðŸ”¸ Bronze â†’ Silver
- Files: `2_silver.py`, `4_silver.py`  
- Cleans nulls, splits columns, casts datatypes  
- Adds derived fields like `Shorttitle`, `duration_minutes`, etc.

### 3. ðŸ§© Lookups
- Files: `3.Lookup notebook.py`, `5_lookupNotebook.py`  
- Adds metadata from dimension tables (e.g., country/rating mappings)

### 4. ðŸ¥‡ Silver â†’ Gold
- File: `DLT notebook - Gold Layer.py`  
- Performs final aggregation for reporting (e.g., counts by type)

### 5. âž• Extras
- `6.weekday.py`: Adds a `weekday` column for release day analysis

---

## ðŸ§© Azure Data Factory Integration

The folder `pipeline1_support_live/` includes:
- `pipeline1.json` â€“ Full ADF pipeline (ForEach, Web Activity, etc.)  
- Linked Services:
  - `DataLake_connection.json`
  - `github_connection.json`
- Datasets:
  - GitHub resources
  - Validation
  - Sink definitions

---

## ðŸš€ How to Run the Pipeline

1. **Deploy ADF pipeline** using the exported JSON  
2. **Upload Netflix CSVs** to Azure Data Lake â†’ `raw` container  
3. **Run Databricks notebooks in this order**:
   - `1.AutoLoader.py`
   - `2_silver.py` and `4_silver.py`
   - `3.Lookup notebook.py` and `5_lookupNotebook.py`
   - `DLT notebook - Gold Layer.py`
4. **Visualize** Gold Layer output using:
   - Power BI
   - Databricks SQL Dashboards

---

## ðŸ“Œ Best Practices Followed

- âœ… Modular notebooks for each pipeline stage  
- âœ… Follows Medallion architecture (Raw â†’ Gold)  
- âœ… Real-time streaming ingestion using AutoLoader  
- âœ… ACID-compliant Delta Lake storage  
- âœ… Parameterized lookups & reusable logic  

---

## ðŸ“Ž Optional Files

- `manifest.mf`: Manifest file (can be ignored for now)

---

## ðŸ™Œ Author

**Srinath Abburi**  
Passionate about cloud data engineering, streaming pipelines, and real-time analytics.

ðŸ“§ Email: [abburisrinath09@gmail.com](mailto:abburisrinath09@gmail.com)  
ðŸ”— LinkedIn: [https://www.linkedin.com/in/abburi-srinath-59a505281/]

---

> ðŸ’¡ Feel free to fork, clone, and explore this project. Contributions are welcome!
